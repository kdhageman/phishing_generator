{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/kh/.virtualenvs/advanced_ml_dtu/lib/python3.6/site-packages (0.4.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install torch  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
    "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
    "!apt-get update -qq 2>&1 > /dev/null\n",
    "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "from oauth2client.client import GoogleCredentials\n",
    "creds = GoogleCredentials.get_application_default()\n",
    "import getpass\n",
    "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
    "vcode = getpass.getpass()\n",
    "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p drive\n",
    "!google-drive-ocamlfuse drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial boilerplate code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Define vocabulary dictionary\n",
    "next_index = 0\n",
    "def get_new_index():\n",
    "  global next_index\n",
    "  index = next_index\n",
    "  next_index += 1\n",
    "  return index\n",
    "\n",
    "vocabulary = defaultdict(get_new_index)\n",
    "\n",
    "class args:\n",
    "    batch_size = 64   \n",
    "    seq_len = 64\n",
    "    train_size = 1000\n",
    "    n_epochs = 1\n",
    "    print_every = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the phishing domains from a file. The file is expected to contain a pickled Python list of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from random import shuffle, seed\n",
    "\n",
    "seed(1)\n",
    "\n",
    "TRAIN_FILE = \"data/phishing_domains.dat\"\n",
    "with open(TRAIN_FILE, 'rb') as f:\n",
    "    data = pickle.load(f)  \n",
    "    \n",
    "# Randomly sample domains\n",
    "shuffle(data)\n",
    "data = data[:args.train_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize data and fill (reverse) vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "all_chars = list(itertools.chain.from_iterable([list(domain.strip()) + ['<eos>'] for domain in data]))\n",
    "tokens = torch.tensor([vocabulary[char] for char in all_chars])\n",
    "vocab_size = len(vocabulary)\n",
    "reverse_vocab = dict([(v,k) for k, v in vocabulary.items()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into batches for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(data, batch_size):\n",
    "    data_size = (len(tokens) // batch_size) * batch_size    \n",
    "    return data[:data_size].reshape(-1, batch_size).contiguous()\n",
    "\n",
    "def iterate_seq(data, seq_len):\n",
    "    for i in range(0, len(data), seq_len):\n",
    "        cur_seq_len = min(seq_len, len(data)-i-1)\n",
    "        samples = data[i  : i+cur_seq_len]\n",
    "        targets = data[i+1: i+cur_seq_len+1]\n",
    "        yield samples, targets\n",
    "\n",
    "train_data = batchify(tokens, args.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, ntoken, ninp, nhid, nlayers):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(ntoken, ninp)\n",
    "        self.rnn = nn.LSTM(ninp, nhid, nlayers)\n",
    "        self.project = nn.Linear(nhid, ntoken)\n",
    "        \n",
    "    def forward(self, input, hidden=None):\n",
    "        embed = self.embedding(input)\n",
    "        rnn_out, new_hidden = self.rnn(embed, hidden)\n",
    "        return self.project(rnn_out), new_hidden\n",
    "    \n",
    "model = RNNModel(vocab_size, 128 ,256, 2).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "lossfn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Loss: 3.2865\n",
      "Loss: 3.3105\n",
      "Loss: 3.3445\n",
      "Loss: 3.2768\n",
      "Loss: 3.3182\n",
      "Loss: 3.2711\n",
      "Loss: 3.3304\n",
      "Loss: 3.3096\n",
      "Loss: 3.2918\n",
      "Loss: 3.3087\n",
      "Loss: 3.3395\n",
      "Loss: 3.3033\n",
      "Loss: 3.2529\n",
      "Loss: 3.3297\n",
      "Loss: 3.3094\n",
      "Loss: 3.3097\n"
     ]
    }
   ],
   "source": [
    "def train(model, train_data):\n",
    "    for epoch in range(1, args.n_epochs+1):\n",
    "        print(\"Epoch: %d\" % epoch)\n",
    "        hidden = None\n",
    "        for i, (samples, targets) in enumerate(iterate_seq(train_data, args.seq_len)):      \n",
    "            output, hidden = model(samples, hidden)\n",
    "            hidden = tuple(h.detach() for h in hidden)\n",
    "            loss = lossfn(output.reshape(-1, vocab_size),\n",
    "                          targets.reshape(-1))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()                      \n",
    "            if i % args.print_every == args.print_every - 1:\n",
    "                print(\"Loss: %.4f\" % loss.item())                \n",
    "\n",
    "train(model, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "advanced_ml_dtu",
   "language": "python",
   "name": "advanced_ml_dtu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
