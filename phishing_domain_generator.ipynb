{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "phishing_domain_generator.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "-Et7DiHmUjx2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6f5862c8-a8df-401d-ddd7-b75e9cb06c00"
      },
      "cell_type": "code",
      "source": [
        "!pip install torch  "
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (0.4.1)\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JsTvT83gUjx8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "if sys.platform == 'linux':\n",
        "  !apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "  !add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "  !apt-get update -qq 2>&1 > /dev/null\n",
        "  !apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "  from google.colab import auth\n",
        "  auth.authenticate_user()\n",
        "  from oauth2client.client import GoogleCredentials\n",
        "  creds = GoogleCredentials.get_application_default()\n",
        "  import getpass\n",
        "  !google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "  vcode = getpass.getpass()\n",
        "  !echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
        "\n",
        "  !mkdir -p data\n",
        "  !google-drive-ocamlfuse data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HeUbABVjUjyB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "torch.manual_seed(1);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bn-_WM_BW1TB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class args:\n",
        "  train_size = 20000\n",
        "  train_batch_size = 64   \n",
        "  \n",
        "  test_size = 10000\n",
        "  test_batch_size = 32\n",
        "  \n",
        "  seq_len = 64\n",
        "  n_epochs = 5\n",
        "  print_every = 20"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XfASdJiEUjyD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Initial boilerplate code"
      ]
    },
    {
      "metadata": {
        "id": "8wmRmrxlUjyD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "# Define vocabulary dictionary\n",
        "next_index = 0\n",
        "def get_new_index():\n",
        "  global next_index\n",
        "  index = next_index\n",
        "  next_index += 1\n",
        "  return index\n",
        "\n",
        "vocabulary = defaultdict(get_new_index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HMW8u3yrUjyF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Load the phishing domains from a file. The file is expected to contain a pickled Python list of strings."
      ]
    },
    {
      "metadata": {
        "id": "H6haIe8pUjyH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from random import shuffle, seed\n",
        "\n",
        "seed(1)\n",
        "\n",
        "TRAIN_FILE = \"data/phishing_domains.dat\"\n",
        "with open(TRAIN_FILE, 'rb') as f:\n",
        "    data = pickle.load(f)  \n",
        "    \n",
        "# Randomly sample domains\n",
        "shuffle(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1TB18b3iUjyJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Tokenize data and fill (reverse) vocabulary"
      ]
    },
    {
      "metadata": {
        "id": "xJVJqJ10UjyJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def tokenize(data):\n",
        "  chars = list(itertools.chain.from_iterable([list(domain.strip()) + ['<eos>'] for domain in data]))\n",
        "  return torch.tensor([vocabulary[char] for char in chars])\n",
        "\n",
        "  \n",
        "train_tokens = tokenize(data[:args.train_size]).to(device)  \n",
        "test_tokens = tokenize(data[args.train_size:args.train_size+args.test_size]).to(device)\n",
        "  \n",
        "vocab_size = len(vocabulary)\n",
        "reverse_vocab = dict([(v,k) for k, v in vocabulary.items()])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "McNHCORHUjyM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Split data into batches for training"
      ]
    },
    {
      "metadata": {
        "id": "8cIcCviNUjyN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def batchify(data, batch_size):\n",
        "    data_size = (len(data) // batch_size) * batch_size    \n",
        "    return data[:data_size].reshape(-1, batch_size).contiguous()\n",
        "\n",
        "def iterate_seq(data, seq_len):\n",
        "    for i in range(0, len(data), seq_len):\n",
        "        cur_seq_len = min(seq_len, len(data)-i-1)\n",
        "        samples = data[i  : i+cur_seq_len]\n",
        "        targets = data[i+1: i+cur_seq_len+1]\n",
        "        yield samples, targets\n",
        "\n",
        "train_data = batchify(train_tokens, args.train_batch_size)\n",
        "test_data = batchify(test_tokens, args.test_batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jSGjNhhkUjyT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Define model"
      ]
    },
    {
      "metadata": {
        "id": "Zdfj5CMQUjyT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class RNNModel(nn.Module):\n",
        "    def __init__(self, ntoken, ninp, nhid, nlayers):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(ntoken, ninp)\n",
        "        self.rnn = nn.LSTM(ninp, nhid, nlayers)\n",
        "        self.project = nn.Linear(nhid, ntoken)\n",
        "        \n",
        "    def forward(self, input, hidden=None):\n",
        "        embed = self.embedding(input)\n",
        "        rnn_out, new_hidden = self.rnn(embed, hidden)\n",
        "        return self.project(rnn_out), new_hidden\n",
        "    \n",
        "model = RNNModel(vocab_size, 32 ,64, 2).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "lossfn = nn.CrossEntropyLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j_iH8plmUjyV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Train model"
      ]
    },
    {
      "metadata": {
        "id": "ZqsUsC6uUjyV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 674
        },
        "outputId": "1713ed73-eaa9-49b0-991e-1040139cf676"
      },
      "cell_type": "code",
      "source": [
        "def evaluate(model, data):  \n",
        "  # validate the model  \n",
        "  total_loss = 0\n",
        "  with torch.no_grad():\n",
        "    hidden = None\n",
        "    for samples, targets in iterate_seq(data, args.seq_len):            \n",
        "      output, hidden = model(samples, hidden)\n",
        "      hidden = tuple(h.detach() for h in hidden)\n",
        "      loss = lossfn(output.reshape(-1, vocab_size),\n",
        "                    targets.reshape(-1))\n",
        "      total_loss += loss\n",
        "  return total_loss / len(data)\n",
        "\n",
        "def train(model, train_data):\n",
        "    test_error = []\n",
        "    \n",
        "    for epoch in range(1, args.n_epochs+1):\n",
        "        # train\n",
        "        model.train()\n",
        "        print(\"Epoch: %d\" % epoch)\n",
        "        hidden = None\n",
        "        for i, (samples, targets) in enumerate(iterate_seq(train_data, args.seq_len)):      \n",
        "            output, hidden = model(samples, hidden)\n",
        "            hidden = tuple(h.detach() for h in hidden)\n",
        "            loss = lossfn(output.reshape(-1, vocab_size),\n",
        "                          targets.reshape(-1))\n",
        "            print(loss)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()                      \n",
        "            if i % args.print_every == args.print_every - 1:\n",
        "                print(\"Loss: %.4f\" % loss.item())     \n",
        "        # performance on test set\n",
        "        model.eval()\n",
        "        test_error.append(evaluate(model, test_data))\n",
        "    return test_error\n",
        "\n",
        "test_error = train(model, train_data)\n"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1\n",
            "tensor(3.2883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "tensor(3.3029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "tensor(3.3519, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "tensor(3.3499, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "tensor(3.3303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "tensor(3.2953, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "tensor(3.3107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "tensor(3.2695, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "tensor(3.3164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "tensor(3.2950, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "tensor(3.2885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "tensor(3.2751, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "tensor(3.3253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "tensor(3.3201, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "tensor(3.3270, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "tensor(3.3294, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "tensor(3.2798, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "tensor(3.2759, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "tensor(3.3075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "tensor(3.2682, device='cuda:0', grad_fn=<NllLossBackward>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-128-ae2a33f49846>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtest_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0mtest_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-128-ae2a33f49846>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_data)\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_every\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_every\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loss: %.4f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m# performance on test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mException\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "7cEjV3pYbepG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "99845e45-c020-471c-948a-e12131f5f32a"
      },
      "cell_type": "code",
      "source": [
        "test_error"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor(0.0521, device='cuda:0'),\n",
              " tensor(0.0521, device='cuda:0'),\n",
              " tensor(0.0521, device='cuda:0'),\n",
              " tensor(0.0521, device='cuda:0'),\n",
              " tensor(0.0521, device='cuda:0')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 126
        }
      ]
    },
    {
      "metadata": {
        "id": "_Acf_PVRe7ge",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}