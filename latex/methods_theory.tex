\section{Methods and theory}
The selected model for this project is a recurrent neural network (RNN).
In contrast to feed forward neural networks, an RNN introduces a loop in the network by passing the output (or hidden information) as input for the next step of the network.
This allows the network to maintain a memory of previous outputs of the network, which makes the network suitable for sequence-based domains, such as time-series data and natural language processing.

Figure \ref{fig:unrolled} depicts how the state of an RNN is fed to the next step.
At each step $i$, an input vector $x_i$ is fed into the network, as well as information from the previous state.
This information passed between steps can take different shapes, such as the output of the previous step, or some hidden state of the previous step.
For $i=0$, this previous information tends to be initialized either randomly or as a default value.
Given the both inputs, the RNN generates an output $h_i$.
The structure from Figure \ref{fig:unrolled} hints that the prediction of output $h_3$ is based on its direct input $x_3$, but also all its preceding inputs $x_0$, $x_1$ and $x_2$.

\begin{figure}[t]
	\centering
	\includegraphics[width=0.6\textwidth]{figures/unrolled.eps} 
	\caption{A generic representation of an RNN. Input $x_i$ and information from the previous step results in output $h_i$.}
	\label{fig:unrolled}
\end{figure}

\subsection{Network design}
Our proposed RNN accepts an encoding of a character and outputs a probability vector, where each element in the output vector represents the probability of a certain character following the input vector.

Figure \ref{fig:rnn} shows the design of the RNN. The network accepts two inputs: the input, and a hidden state of the previous time step. 
The hidden state is initialized as a vector of zeros for the first step.
Both the input and hidden vectors are one-dimensional. 
The input vector is a one-hot encoding of a single character. 
Therefore, the size of the input vector is defined by the size of the alphabet (i.e. the list of all possible characters).
The size of the hidden vector is one of the hyper-parameters of the network.
The concatenation of the input and hidden vectors is fed to two linear transformation layers (input-to-output {\tt i2o} and input-to-hidden {\tt i2h}). 
The output of {\tt i2h} is the hidden state for the next step of the network.

Both outputs are concatenated and fed in yet another linear transformation layer (output-to-output {\tt o2o}).
The output size is equal to the alphabet size (i.e. the number of unique encountered characters in the training set), such that the output layer can be used as a classifier for the predicted character (i.e. each index in the output layer is the probability for a certain character in the alphabet).
A dropout layer sets values of the output vector to zero with a probability of $p$ (again, a hyper parameter).
This layer acts as a countermeasure against overfitting on the training data.
A last (log) softmax converts the output vector to a logarithmic probability density.

\begin{figure}[t]
	\centering
	\includegraphics[width=0.75\textwidth]{figures/rnn.eps} 
	\caption{The design of the neural network. The grey boxes are input- and output vectors, the blue boxes represent linear transformation layers and the green boxes are vector transformations
	 applied to the output of the network.}
	\label{fig:rnn}
\end{figure}

\subsubsection{Hyper parameters}
The Adam algorithm was used for the optimization.
For computation of the loss, the negative log likelihood function was used. 
Through experimentation, the size of the hidden layers in the RNN were set to 128 nodes.
Similarly, the learning rate of the optimizer was set to 0.001.
The dropout probability was set to 0.1, the default value in PyTorch.